{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46665e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (2119, 2)\n",
      "Testing set shape: (530, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('tokenized_data.csv', encoding='utf-8')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split your dataset into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the training and testing sets\n",
    "print(\"Training set shape:\", train_df.shape)\n",
    "print(\"Testing set shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "607a707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_sentences = df['Hindi_Tokens']\n",
    "telugu_sentences = df['Telugu_Tokens']\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "hindi_train, hindi_val, telugu_train, telugu_val = train_test_split(\n",
    "    hindi_sentences, telugu_sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now you have the following variables:\n",
    "# - hindi_train: List of Hindi sentences for training\n",
    "# - hindi_val: List of Hindi sentences for validation\n",
    "# - telugu_train: List of corresponding Telugu sentences for training\n",
    "# - telugu_val: List of corresponding Telugu sentences for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d9c79a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi Training set shape: (2119,)\n",
      "Hindi Val set shape: (530,)\n",
      "Telugu Training set shape: (2119,)\n",
      "Telugu Val set shape: (530,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Hindi Training set shape:\", hindi_train.shape)\n",
    "print(\"Hindi Val set shape:\", hindi_val.shape)\n",
    "print(\"Telugu Training set shape:\", telugu_train.shape)\n",
    "print(\"Telugu Val set shape:\", telugu_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0303ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "hindi_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "hindi_tokenizer.fit_on_texts(hindi_sentences)\n",
    "hindi_vocab_size = len(hindi_tokenizer.word_index) + 1\n",
    "hindi_sequences = hindi_tokenizer.texts_to_sequences(hindi_sentences)\n",
    "\n",
    "# Tokenize Telugu sentences\n",
    "telugu_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "telugu_tokenizer.fit_on_texts(telugu_sentences)\n",
    "telugu_vocab_size = len(telugu_tokenizer.word_index) + 1\n",
    "telugu_sequences = telugu_tokenizer.texts_to_sequences(telugu_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e97bb21b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13424, 7249)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(telugu_tokenizer.word_index) + 1, len(hindi_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81900dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = max(len(seq) for seq in hindi_sequences)\n",
    "hindi_sequences = pad_sequences(hindi_sequences, maxlen=max_sequence_length, padding=\"post\")\n",
    "telugu_sequences = pad_sequences(telugu_sequences, maxlen=max_sequence_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a497d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Input\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_sequence_length,))\n",
    "encoder_embedding = Embedding(hindi_vocab_size, 256, input_length=max_sequence_length)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_sequence_length,))\n",
    "decoder_embedding = Embedding(telugu_vocab_size, 256, input_length=max_sequence_length)(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(telugu_vocab_size, activation=\"softmax\")\n",
    "output = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Create the model\n",
    "model = Model([encoder_inputs, decoder_inputs], output)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af4850e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 190)]                0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 190)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 190, 256)             1855744   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 190, 256)             3436544   ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 256),                525312    ['embedding[0][0]']           \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, 190, 256),           525312    ['embedding_1[0][0]',         \n",
      "                              (None, 256),                           'lstm[0][1]',                \n",
      "                              (None, 256)]                           'lstm[0][2]']                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 190, 13424)           3449968   ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9792880 (37.36 MB)\n",
      "Trainable params: 9792880 (37.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44f09a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "telugu_sequences = np.random.randint(0, 2, size=(64,190))  # Replace with your data\n",
    "telugu_vocab_size = 13424  # Replace with your vocabulary size\n",
    "\n",
    "# Convert target data to float16\n",
    "target_data = tf.keras.utils.to_categorical(telugu_sequences, num_classes=telugu_vocab_size, dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42e4c72f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 2119, 64\n  y sizes: 64\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m target_data \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mto_categorical(telugu_sequences, num_classes\u001b[38;5;241m=\u001b[39mtelugu_vocab_size, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39mfit([hindi_sequences, telugu_sequences], target_data, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1950\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1943\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1944\u001b[0m         label,\n\u001b[0;32m   1945\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m   1946\u001b[0m             \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1947\u001b[0m         ),\n\u001b[0;32m   1948\u001b[0m     )\n\u001b[0;32m   1949\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1950\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 2119, 64\n  y sizes: 64\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "# Prepare data for training\n",
    "target_data = tf.keras.utils.to_categorical(telugu_sequences, num_classes=telugu_vocab_size, dtype='float32')\n",
    "\n",
    "# Train the model\n",
    "model.fit([hindi_sequences, telugu_sequences], target_data, epochs=5, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ec7774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
